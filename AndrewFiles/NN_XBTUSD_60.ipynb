{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved to: C:\\Users\\abrau\\uvic\\seng474\\project\\CryptoAI\\AndrewFiles\\XBTUSD_60_with_features.csv\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory containing preprocessor.py to the Python path\n",
    "script_dir = r\"C:\\Users\\abrau\\uvic\\seng474\\project\\CryptoAI\\AndrewFiles\"\n",
    "sys.path.append(script_dir)\n",
    "\n",
    "# Import the preprocessor module\n",
    "import preprocessor\n",
    "\n",
    "# Define file paths\n",
    "input_filepath = r\"C:\\Users\\abrau\\uvic\\seng474\\project\\CryptoAI\\Kraken_OHLCVT\\XBTUSD_60.csv\"\n",
    "output_directory = r\"C:\\Users\\abrau\\uvic\\seng474\\project\\CryptoAI\\AndrewFiles\"\n",
    "output_filepath = os.path.join(output_directory, \"XBTUSD_60_with_features.csv\")\n",
    "\n",
    "# Process the file using the preprocessor module\n",
    "preprocessor.process_file(input_filepath, output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abrau\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 73ms/step - loss: 8.7435e-04 - mae: 0.0166 - mse: 8.7435e-04 - val_loss: 0.0014 - val_mae: 0.0312 - val_mse: 0.0014\n",
      "Epoch 2/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 68ms/step - loss: 1.9173e-04 - mae: 0.0077 - mse: 1.9173e-04 - val_loss: 0.0026 - val_mae: 0.0462 - val_mse: 0.0026\n",
      "Epoch 3/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 53ms/step - loss: 1.3583e-04 - mae: 0.0065 - mse: 1.3583e-04 - val_loss: 0.0030 - val_mae: 0.0472 - val_mse: 0.0030\n",
      "Epoch 4/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 54ms/step - loss: 1.1537e-04 - mae: 0.0058 - mse: 1.1537e-04 - val_loss: 0.0024 - val_mae: 0.0434 - val_mse: 0.0024\n",
      "Epoch 5/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 67ms/step - loss: 9.5595e-05 - mae: 0.0054 - mse: 9.5595e-05 - val_loss: 0.0027 - val_mae: 0.0484 - val_mse: 0.0027\n",
      "Epoch 6/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 72ms/step - loss: 6.6940e-05 - mae: 0.0046 - mse: 6.6940e-05 - val_loss: 0.0031 - val_mae: 0.0495 - val_mse: 0.0031\n",
      "Epoch 7/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 119ms/step - loss: 5.3332e-05 - mae: 0.0043 - mse: 5.3332e-05 - val_loss: 0.0081 - val_mae: 0.0811 - val_mse: 0.0081\n",
      "Epoch 8/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 69ms/step - loss: 4.6358e-05 - mae: 0.0039 - mse: 4.6358e-05 - val_loss: 0.0099 - val_mae: 0.0931 - val_mse: 0.0099\n",
      "Epoch 9/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 87ms/step - loss: 5.7459e-05 - mae: 0.0043 - mse: 5.7459e-05 - val_loss: 0.0065 - val_mae: 0.0743 - val_mse: 0.0065\n",
      "Epoch 10/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5698s\u001b[0m 3s/step - loss: 4.3419e-05 - mae: 0.0037 - mse: 4.3419e-05 - val_loss: 0.0106 - val_mae: 0.0955 - val_mse: 0.0106\n",
      "Epoch 11/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 86ms/step - loss: 3.7023e-05 - mae: 0.0034 - mse: 3.7023e-05 - val_loss: 0.0075 - val_mae: 0.0786 - val_mse: 0.0075\n",
      "Epoch 12/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 66ms/step - loss: 3.3144e-05 - mae: 0.0033 - mse: 3.3144e-05 - val_loss: 0.0129 - val_mae: 0.1038 - val_mse: 0.0129\n",
      "Epoch 13/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 45ms/step - loss: 3.0060e-05 - mae: 0.0031 - mse: 3.0060e-05 - val_loss: 0.0097 - val_mae: 0.0907 - val_mse: 0.0097\n",
      "Epoch 14/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 49ms/step - loss: 3.1329e-05 - mae: 0.0031 - mse: 3.1329e-05 - val_loss: 0.0122 - val_mae: 0.1015 - val_mse: 0.0122\n",
      "Epoch 15/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 43ms/step - loss: 2.6783e-05 - mae: 0.0029 - mse: 2.6783e-05 - val_loss: 0.0074 - val_mae: 0.0786 - val_mse: 0.0074\n",
      "Epoch 16/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 43ms/step - loss: 2.8921e-05 - mae: 0.0030 - mse: 2.8921e-05 - val_loss: 0.0098 - val_mae: 0.0898 - val_mse: 0.0098\n",
      "Epoch 17/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 42ms/step - loss: 2.5544e-05 - mae: 0.0028 - mse: 2.5544e-05 - val_loss: 0.0103 - val_mae: 0.0951 - val_mse: 0.0103\n",
      "Epoch 18/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 43ms/step - loss: 2.4365e-05 - mae: 0.0028 - mse: 2.4365e-05 - val_loss: 0.0099 - val_mae: 0.0924 - val_mse: 0.0099\n",
      "Epoch 19/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 39ms/step - loss: 2.3648e-05 - mae: 0.0028 - mse: 2.3648e-05 - val_loss: 0.0100 - val_mae: 0.0926 - val_mse: 0.0100\n",
      "Epoch 20/20\n",
      "\u001b[1m1706/1706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 39ms/step - loss: 2.6771e-05 - mae: 0.0028 - mse: 2.6771e-05 - val_loss: 0.0075 - val_mae: 0.0764 - val_mse: 0.0075\n",
      "loss: 2.5330495191155933e-05\n",
      "mae: 0.002743612742051482\n",
      "mse: 2.5330495191155933e-05\n",
      "val_loss: 0.007525171153247356\n",
      "val_mae: 0.07636367529630661\n",
      "val_mse: 0.007525171153247356\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the training data\n",
    "train_filepath = r'C:\\Users\\abrau\\uvic\\seng474\\project\\CryptoAI\\AndrewFiles\\XBTUSD_60_with_features_train.csv'\n",
    "train_df = pd.read_csv(train_filepath)\n",
    "\n",
    "# Drop the timestamp column\n",
    "train_df = train_df.drop(columns=['Timestamp'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "# Extract all features (excluding timestamp)\n",
    "train_data = train_df.values  # Use all columns\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "\n",
    "# Create sequences for LSTM\n",
    "sequence_length = 50  # Use past 50 timesteps to predict next\n",
    "X_train, y_train = [], []\n",
    "for i in range(len(train_data_scaled) - sequence_length):\n",
    "    X_train.append(train_data_scaled[i:i+sequence_length])\n",
    "    y_train.append(train_data_scaled[i+sequence_length, 3])  # Predict closing price (column index 3)\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(sequence_length, train_data.shape[1])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'mse'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Print accuracy metrics\n",
    "for key in history.history.keys():\n",
    "    print(f\"{key}: {history.history[key][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m533/533\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.0045 - mae: 0.0495 - mse: 0.0045\n",
      "\n",
      "Test Loss: 0.0160\n",
      "Test MAE: 0.0990\n",
      "Test MSE: 0.0160\n",
      "\u001b[1m533/533\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step\n",
      "Predicted: 0.2460, Actual: 0.2773\n",
      "Predicted: 0.2459, Actual: 0.2776\n",
      "Predicted: 0.2469, Actual: 0.2775\n",
      "Predicted: 0.2464, Actual: 0.2782\n",
      "Predicted: 0.2474, Actual: 0.2780\n",
      "Predicted: 0.2469, Actual: 0.2777\n",
      "Predicted: 0.2463, Actual: 0.2782\n",
      "Predicted: 0.2473, Actual: 0.2779\n",
      "Predicted: 0.2466, Actual: 0.2778\n",
      "Predicted: 0.2468, Actual: 0.2777\n"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "test_filepath = r'C:\\Users\\abrau\\uvic\\seng474\\project\\CryptoAI\\AndrewFiles\\XBTUSD_60_with_features_test.csv'\n",
    "test_df = pd.read_csv(test_filepath)\n",
    "\n",
    "# Drop the timestamp column\n",
    "test_df = test_df.drop(columns=['Timestamp'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "test_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# Extract all features (excluding timestamp)\n",
    "test_data = test_df.values  # Use all columns\n",
    "\n",
    "# Normalize test data using the same scaler as training data\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Create sequences for LSTM\n",
    "X_test, y_test = [], []\n",
    "for i in range(len(test_data_scaled) - sequence_length):\n",
    "    X_test.append(test_data_scaled[i:i+sequence_length])\n",
    "    y_test.append(test_data_scaled[i+sequence_length, 3])  # Predict closing price (column index 3)\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_mae, test_mse = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Optionally, you can compare the predicted vs actual values\n",
    "for i in range(10):  # Print the first 10 predictions\n",
    "    print(f\"Predicted: {y_pred[i][0]:.4f}, Actual: {y_test[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m533/533\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.0045 - mae: 0.0495 - mse: 0.0045\n",
      "\n",
      "Test Loss: 0.0160\n",
      "Test MAE: 0.0990\n",
      "Test MSE: 0.0160\n",
      "\u001b[1m533/533\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (17035,1) doesn't match the broadcast shape (17035,19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m y_pred_reshaped \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Inverse transform the scaled values to original units\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m y_test_original \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_reshaped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m y_pred_original \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_reshaped)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Recalculate MAE in original units\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abrau\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:547\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    541\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    543\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    544\u001b[0m     X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy, dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m )\n\u001b[1;32m--> 547\u001b[0m \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_\u001b[49m\n\u001b[0;32m    548\u001b[0m X \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (17035,1) doesn't match the broadcast shape (17035,19)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the test data\n",
    "test_filepath = r'C:\\Users\\abrau\\uvic\\seng474\\project\\CryptoAI\\AndrewFiles\\XBTUSD_60_with_features_test.csv'\n",
    "test_df = pd.read_csv(test_filepath)\n",
    "\n",
    "# Drop the timestamp column\n",
    "test_df = test_df.drop(columns=['Timestamp'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "test_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# Extract all features (excluding timestamp)\n",
    "test_data = test_df.values  # Use all columns\n",
    "\n",
    "# Normalize test data using the same scaler as training data\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Create sequences for LSTM\n",
    "X_test, y_test = [], []\n",
    "for i in range(len(test_data_scaled) - sequence_length):\n",
    "    X_test.append(test_data_scaled[i:i+sequence_length])\n",
    "    y_test.append(test_data_scaled[i+sequence_length, 3])  # Predict closing price (column index 3)\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_mae, test_mse = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Reshape y_test and y_pred for inverse_transform\n",
    "# y_test and y_pred are 1D arrays, so we need to reshape them to 2D arrays\n",
    "y_test_reshaped = y_test.reshape(-1, 1)\n",
    "y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "\n",
    "# Inverse transform the scaled values to original units\n",
    "y_test_original = scaler.inverse_transform(y_test_reshaped)\n",
    "y_pred_original = scaler.inverse_transform(y_pred_reshaped)\n",
    "\n",
    "# Recalculate MAE in original units\n",
    "mae_original = np.mean(np.abs(y_test_original - y_pred_original))\n",
    "print(f\"\\nMAE in original units: {mae_original:.4f}\")\n",
    "\n",
    "# Optionally, you can compare the predicted vs actual values in original units\n",
    "for i in range(10):  # Print the first 10 predictions\n",
    "    print(f\"Predicted: {y_pred_original[i][0]:.4f}, Actual: {y_test_original[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
