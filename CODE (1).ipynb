{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SENG 474 Project - Initial Model"
      ],
      "metadata": {
        "id": "C8NfS1aIZR-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Model"
      ],
      "metadata": {
        "id": "7XDvf89guqm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# Filepath\n",
        "filepath = \"Kraken_OHLCVT/XBTUSD_15.csv\"\n",
        "\n",
        "# Threshold for determining if a coin went up\n",
        "threshold = 0\n",
        "\n",
        "#############################################\n",
        "# 1) Load CSV & Rename columns\n",
        "#############################################\n",
        "df = pd.read_csv(filepath)\n",
        "df.columns = [\"Timestamp\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Trades\"]\n",
        "\n",
        "#############################################\n",
        "# 2) Define feature-engineering functions\n",
        "#############################################\n",
        "def add_datetime_features(df):\n",
        "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"s\")  # Convert to datetime\n",
        "\n",
        "    df[\"Weekday\"] = df[\"Timestamp\"].dt.weekday  # 0=Mon, 6=Sun\n",
        "    df[\"Day\"] = df[\"Timestamp\"].dt.day\n",
        "    df[\"Year\"] = df[\"Timestamp\"].dt.year.astype(float)\n",
        "\n",
        "    # Time of day\n",
        "    df[\"TOD\"] = df[\"Timestamp\"].dt.hour + df[\"Timestamp\"].dt.minute / 60.0\n",
        "\n",
        "    # Cyclical encoding for Month\n",
        "    month = df[\"Timestamp\"].dt.month\n",
        "    df[\"Month_Sin\"] = np.sin(2 * np.pi * month / 12.0)\n",
        "    df[\"Month_Cos\"] = np.cos(2 * np.pi * month / 12.0)\n",
        "\n",
        "    # Cyclical encoding for TOD (24 hours in a day)\n",
        "    df[\"TOD_Sin\"] = np.sin(2 * np.pi * df[\"TOD\"] / 24.0)\n",
        "    df[\"TOD_Cos\"] = np.cos(2 * np.pi * df[\"TOD\"] / 24.0)\n",
        "\n",
        "    # Drop the original columns\n",
        "    df.drop(columns=[\"Timestamp\", \"TOD\"], inplace=True)\n",
        "    return df\n",
        "\n",
        "def compute_rsi(series, period=14):\n",
        "    delta = series.diff().dropna()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / (loss + 1e-10)  # Avoid division by zero\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def add_features(df, threshold):\n",
        "    df[\"SMA_10\"] = df[\"Close\"].rolling(window=10).mean()\n",
        "    df[\"SMA_50\"] = df[\"Close\"].rolling(window=50).mean()\n",
        "    df[\"RSI_14\"] = compute_rsi(df[\"Close\"])\n",
        "    df[\"Return\"] = df[\"Close\"].pct_change()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    df[\"Middle_Band\"] = df[\"Close\"].rolling(window=20).mean()\n",
        "    stddev = df[\"Close\"].rolling(window=20).std()\n",
        "    df[\"Upper_Band\"] = df[\"Middle_Band\"] + (2 * stddev)\n",
        "    df[\"Lower_Band\"] = df[\"Middle_Band\"] - (2 * stddev)\n",
        "\n",
        "    # Return_Signal in {-1, 0, 1}\n",
        "    df[\"Return_Signal\"] = df[\"Return\"].apply(\n",
        "        lambda x: 1 if x > threshold else (0 if x >= 0 else -1)\n",
        "    )\n",
        "\n",
        "    # Add time-based features (which won't leak future data if done carefully)\n",
        "    df = add_datetime_features(df)\n",
        "    return df\n",
        "\n",
        "#############################################\n",
        "# 3) Split into Train/Val/Test BEFORE fitting scaler\n",
        "#############################################\n",
        "# e.g., 70% train, 15% val, 15% test\n",
        "df = df.reset_index(drop=True)\n",
        "n = len(df)\n",
        "train_end = int(0.7 * n)\n",
        "val_end = int(0.85 * n)\n",
        "\n",
        "train_df = df.iloc[:train_end].copy()\n",
        "val_df = df.iloc[train_end:val_end].copy()\n",
        "test_df = df.iloc[val_end:].copy()\n",
        "\n",
        "#############################################\n",
        "# 4) Add features to the full dataframe,\n",
        "#    after split to void leakage\n",
        "#############################################\n",
        "train_df = add_features(train_df, threshold)\n",
        "val_df = add_features(val_df, threshold)\n",
        "test_df = add_features(test_df, threshold)\n",
        "\n",
        "#############################################\n",
        "# 5) Fill NaNs using means of respective sets\n",
        "#############################################\n",
        "train_means = train_df.mean(numeric_only=True)\n",
        "train_df.fillna(train_means, inplace=True)\n",
        "val_df.fillna(train_means, inplace=True)\n",
        "test_df.fillna(train_means, inplace=True)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# 6) Scale using only the TRAIN set\n",
        "#############################################\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Exclude the label \"Return_Signal\" from scaling\n",
        "exclude_cols = [\"Return_Signal\", \"Month_Sin\", \"Month_Cos\", \"TOD_Sin\", \"TOD_Cos\"]\n",
        "features_to_scale = [col for col in train_df.columns if col not in exclude_cols]\n",
        "\n",
        "# Fit scaler on train\n",
        "scaler.fit(train_df[features_to_scale])\n",
        "\n",
        "# Transform train, val, test\n",
        "train_df[features_to_scale] = scaler.transform(train_df[features_to_scale])\n",
        "val_df[features_to_scale]  = scaler.transform(val_df[features_to_scale])\n",
        "test_df[features_to_scale] = scaler.transform(test_df[features_to_scale])\n",
        "\n",
        "# Save scaler\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "#############################################\n",
        "# 7) Reorder columns so that Return_Signal is last\n",
        "#############################################\n",
        "def reorder_columns(df):\n",
        "    cols = list(df.columns)\n",
        "    cols.remove(\"Return_Signal\")\n",
        "    cols.append(\"Return_Signal\")\n",
        "    return df[cols]\n",
        "\n",
        "train_df = reorder_columns(train_df)\n",
        "val_df = reorder_columns(val_df)\n",
        "test_df = reorder_columns(test_df)\n",
        "\n",
        "#############################################\n",
        "# 8) Convert to NumPy & Shift labels {-1,0,1} -> {0,1,2}\n",
        "#############################################\n",
        "def to_numpy_and_shift_labels(df):\n",
        "    data = df.to_numpy()\n",
        "    label_idx = df.columns.get_loc(\"Return_Signal\")\n",
        "\n",
        "    # Single pass: -1 -> 0, 0 -> 1, 1 -> 2\n",
        "    data[:, label_idx] = np.where(\n",
        "        data[:, label_idx] == -1, 0,\n",
        "        np.where(\n",
        "            data[:, label_idx] == 0, 1,\n",
        "            2\n",
        "        )\n",
        "    )\n",
        "    return data\n",
        "\n",
        "\n",
        "train_data = to_numpy_and_shift_labels(train_df)\n",
        "val_data = to_numpy_and_shift_labels(val_df)\n",
        "test_data = to_numpy_and_shift_labels(test_df)\n",
        "\n",
        "#############################################\n",
        "# 9) Create sequences (X, y) from each split\n",
        "#############################################\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        # X: the past seq_length rows, all columns except the label\n",
        "        X.append(data[i : i + seq_length, :-1])\n",
        "        # y: the label in the next row\n",
        "        y.append(data[i + seq_length, -1])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 30\n",
        "X_train, y_train = create_sequences(train_data, seq_length)\n",
        "X_val,   y_val   = create_sequences(val_data,   seq_length)\n",
        "X_test,  y_test  = create_sequences(test_data,  seq_length)\n",
        "\n",
        "#############################################\n",
        "# 10) Build LSTM Classification Model\n",
        "#############################################\n",
        "num_features = X_train.shape[2]\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(50, return_sequences=True, input_shape=(seq_length, num_features)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(50, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(25, activation=\"relu\"),\n",
        "    Dense(3, activation=\"softmax\")  # 3 classes: 0=down, 1=neutral, 2=up\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",  # for integer labels\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#############################################\n",
        "# 11) Early Stopping & Training\n",
        "#############################################\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#############################################\n",
        "# 11) Evaluate on Test Set\n",
        "#############################################\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Classification report and confusion matrix\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHmVa4Eruo6c",
        "outputId": "21569355-0b99-41cb-a87e-a0edcade30fe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 8ms/step - accuracy: 0.4877 - loss: 0.7809 - val_accuracy: 0.5019 - val_loss: 0.7235\n",
            "Epoch 2/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 8ms/step - accuracy: 0.4918 - loss: 0.7764 - val_accuracy: 0.5019 - val_loss: 0.7251\n",
            "Epoch 3/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 8ms/step - accuracy: 0.4941 - loss: 0.7752 - val_accuracy: 0.4920 - val_loss: 0.7239\n",
            "Epoch 4/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 8ms/step - accuracy: 0.4954 - loss: 0.7739 - val_accuracy: 0.5019 - val_loss: 0.7238\n",
            "Epoch 5/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 8ms/step - accuracy: 0.4913 - loss: 0.7738 - val_accuracy: 0.5019 - val_loss: 0.7261\n",
            "Epoch 6/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 8ms/step - accuracy: 0.4933 - loss: 0.7728 - val_accuracy: 0.5019 - val_loss: 0.7239\n",
            "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.4935 - loss: 0.8334\n",
            "Test Loss: 0.7988221049308777\n",
            "Test Accuracy: 0.4989818334579468\n",
            "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00     22951\n",
            "         1.0       0.00      0.00      0.00       915\n",
            "         2.0       0.50      1.00      0.67     23769\n",
            "\n",
            "    accuracy                           0.50     47635\n",
            "   macro avg       0.17      0.33      0.22     47635\n",
            "weighted avg       0.25      0.50      0.33     47635\n",
            "\n",
            "Confusion Matrix:\n",
            "[[    0     0 22951]\n",
            " [    0     0   915]\n",
            " [    0     0 23769]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Yf-5UEdb1BZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"Train distribution:\", Counter(y_train))\n",
        "print(\"Validation distribution:\", Counter(y_val))\n",
        "print(\"Test distribution:\", Counter(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8bIdo3H2fzi",
        "outputId": "2add8c3c-24ce-49ac-85a4-bee8b8dff7ee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train distribution: Counter({2.0: 110151, 0.0: 106575, 1.0: 5679})\n",
            "Validation distribution: Counter({2.0: 23907, 0.0: 23470, 1.0: 258})\n",
            "Test distribution: Counter({2.0: 23769, 0.0: 22951, 1.0: 915})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We retry with class weights:"
      ],
      "metadata": {
        "id": "aq6MndqI0hAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjCgmD2i12zl",
        "outputId": "2ab413c2-ca82-4a5d-cae7-dfa7d0bde4f8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      1.00      0.98     45837\n",
            "         2.0       0.00      0.00      0.00      1798\n",
            "\n",
            "    accuracy                           0.96     47635\n",
            "   macro avg       0.48      0.50      0.49     47635\n",
            "weighted avg       0.93      0.96      0.94     47635\n",
            "\n",
            "Confusion Matrix:\n",
            "[[45837     0]\n",
            " [ 1798     0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "2GD4ZYqP8fm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression version"
      ],
      "metadata": {
        "id": "TsszCsoI8g1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Filepath\n",
        "filepath = \"Kraken_OHLCVT/XBTUSD_15.csv\"\n",
        "\n",
        "#############################################\n",
        "# 1) Load CSV & Rename columns\n",
        "#############################################\n",
        "df = pd.read_csv(filepath)\n",
        "df.columns = [\"Timestamp\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Trades\"]\n",
        "\n",
        "#############################################\n",
        "# 2) Define feature-engineering functions\n",
        "#############################################\n",
        "def add_datetime_features(df):\n",
        "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"s\")  # Convert to datetime\n",
        "\n",
        "    df[\"Weekday\"] = df[\"Timestamp\"].dt.weekday  # 0=Mon, 6=Sun\n",
        "    df[\"Day\"] = df[\"Timestamp\"].dt.day\n",
        "    df[\"Year\"] = df[\"Timestamp\"].dt.year.astype(float)\n",
        "\n",
        "    # Time of day in hours + fraction\n",
        "    df[\"TOD\"] = df[\"Timestamp\"].dt.hour + df[\"Timestamp\"].dt.minute / 60.0\n",
        "\n",
        "    # Cyclical encoding for Month (12 months)\n",
        "    month = df[\"Timestamp\"].dt.month\n",
        "    df[\"Month_Sin\"] = np.sin(2 * np.pi * month / 12.0)\n",
        "    df[\"Month_Cos\"] = np.cos(2 * np.pi * month / 12.0)\n",
        "\n",
        "    # Cyclical encoding for TOD (24 hours)\n",
        "    df[\"TOD_Sin\"] = np.sin(2 * np.pi * df[\"TOD\"] / 24.0)\n",
        "    df[\"TOD_Cos\"] = np.cos(2 * np.pi * df[\"TOD\"] / 24.0)\n",
        "\n",
        "    # Drop the original Timestamp & TOD columns\n",
        "    df.drop(columns=[\"Timestamp\", \"TOD\"], inplace=True)\n",
        "    return df\n",
        "\n",
        "def compute_rsi(series, period=14):\n",
        "    delta = series.diff().dropna()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / (loss + 1e-10)  # Avoid division by zero\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def add_features(df):\n",
        "    # Basic indicators\n",
        "    df[\"SMA_10\"] = df[\"Close\"].rolling(window=10).mean()\n",
        "    df[\"SMA_50\"] = df[\"Close\"].rolling(window=50).mean()\n",
        "    df[\"RSI_14\"] = compute_rsi(df[\"Close\"])\n",
        "    df[\"Return\"] = df[\"Close\"].pct_change()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    df[\"Middle_Band\"] = df[\"Close\"].rolling(window=20).mean()\n",
        "    stddev = df[\"Close\"].rolling(window=20).std()\n",
        "    df[\"Upper_Band\"] = df[\"Middle_Band\"] + (2 * stddev)\n",
        "    df[\"Lower_Band\"] = df[\"Middle_Band\"] - (2 * stddev)\n",
        "\n",
        "    # Add time-based features\n",
        "    df = add_datetime_features(df)\n",
        "\n",
        "    # For regression: predict the *next-step Close* price\n",
        "    # Shift the 'Close' by -1 step to get the future price\n",
        "    df[\"Target\"] = df[\"Close\"].shift(-1)\n",
        "\n",
        "    return df\n",
        "\n",
        "#############################################\n",
        "# 3) Split into Train/Val/Test BEFORE feature engineering\n",
        "#############################################\n",
        "# We'll do a chronological split: 70% / 15% / 15%\n",
        "df = df.reset_index(drop=True)\n",
        "n = len(df)\n",
        "train_end = int(0.7 * n)\n",
        "val_end = int(0.85 * n)\n",
        "\n",
        "train_df = df.iloc[:train_end].copy()\n",
        "val_df = df.iloc[train_end:val_end].copy()\n",
        "test_df = df.iloc[val_end:].copy()\n",
        "\n",
        "#############################################\n",
        "# 4) Add features\n",
        "#############################################\n",
        "train_df = add_features(train_df)\n",
        "val_df = add_features(val_df)\n",
        "test_df = add_features(test_df)\n",
        "\n",
        "#############################################\n",
        "# 5) Fill NaNs (rolling features & shift cause NaNs)\n",
        "#############################################\n",
        "train_means = train_df.mean(numeric_only=True)\n",
        "train_df.fillna(train_means, inplace=True)\n",
        "val_df.fillna(train_means, inplace=True)\n",
        "test_df.fillna(train_means, inplace=True)\n",
        "\n",
        "#############################################\n",
        "# 6) Scale using only the TRAIN set\n",
        "#############################################\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Include 'Target' in your scaling:\n",
        "exclude_cols = [\"Month_Sin\", \"Month_Cos\", \"TOD_Sin\", \"TOD_Cos\"]  # remove 'Target' from exclude\n",
        "features_to_scale = [col for col in train_df.columns if col not in exclude_cols]\n",
        "\n",
        "# Then fit/transform just like normal\n",
        "scaler.fit(train_df[features_to_scale])\n",
        "train_df[features_to_scale] = scaler.transform(train_df[features_to_scale])\n",
        "val_df[features_to_scale]   = scaler.transform(val_df[features_to_scale])\n",
        "test_df[features_to_scale]  = scaler.transform(test_df[features_to_scale])\n",
        "\n",
        "# Now your 'Target' is in [0,1], so MSE will be in a more typical range.\n",
        "\n",
        "\n",
        "# Transform train, val, test\n",
        "train_df[features_to_scale] = scaler.transform(train_df[features_to_scale])\n",
        "val_df[features_to_scale]  = scaler.transform(val_df[features_to_scale])\n",
        "test_df[features_to_scale] = scaler.transform(test_df[features_to_scale])\n",
        "\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "#############################################\n",
        "# 7) Reorder columns so 'Target' is last\n",
        "#############################################\n",
        "def reorder_columns(df):\n",
        "    cols = list(df.columns)\n",
        "    cols.remove(\"Target\")\n",
        "    cols.append(\"Target\")\n",
        "    return df[cols]\n",
        "\n",
        "train_df = reorder_columns(train_df)\n",
        "val_df = reorder_columns(val_df)\n",
        "test_df = reorder_columns(test_df)\n",
        "\n",
        "#############################################\n",
        "# 8) Convert to NumPy\n",
        "#############################################\n",
        "def to_numpy_array(df):\n",
        "    return df.to_numpy()\n",
        "\n",
        "train_data = to_numpy_array(train_df)\n",
        "val_data   = to_numpy_array(val_df)\n",
        "test_data  = to_numpy_array(test_df)\n",
        "\n",
        "#############################################\n",
        "# 9) Create sequences (X, y) for regression\n",
        "# X is the past seq_length rows; y is the next-step 'Target'\n",
        "#############################################\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        # X: the past seq_length rows, all columns except the last ('Target')\n",
        "        X.append(data[i : i + seq_length, :-1])\n",
        "        # y: the 'Target' in the next row\n",
        "        y.append(data[i + seq_length, -1])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 30\n",
        "X_train, y_train = create_sequences(train_data, seq_length)\n",
        "X_val,   y_val   = create_sequences(val_data,   seq_length)\n",
        "X_test,  y_test  = create_sequences(test_data,  seq_length)\n",
        "\n",
        "#############################################\n",
        "# 10) Build LSTM Regression Model\n",
        "#############################################\n",
        "num_features = X_train.shape[2]\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(50, return_sequences=True, input_shape=(seq_length, num_features)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(50, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(25, activation=\"relu\"),\n",
        "    Dense(1)  # single value output for regression\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"mse\",       # MSE loss for regression\n",
        "    metrics=[\"mae\"]   # Track MAE as well\n",
        ")\n",
        "\n",
        "#############################################\n",
        "# 11) Early Stopping & Training\n",
        "#############################################\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#############################################\n",
        "# 12) Evaluate on Test Set (Regression)\n",
        "#############################################\n",
        "mse, mae = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test MSE:\", mse)\n",
        "print(\"Test MAE:\", mae)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute metrics manually\n",
        "mse_manual = mean_squared_error(y_test, y_pred)\n",
        "mae_manual = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(\"Manual MSE:\", mse_manual)\n",
        "print(\"Manual MAE:\", mae_manual)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDYU5IRb8i_g",
        "outputId": "d5fabe34-7bce-494d-96f9-9b7d9662869c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 8ms/step - loss: 0.0013 - mae: 0.0077 - val_loss: 1.6653e-08 - val_mae: 1.2903e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 8ms/step - loss: 8.0495e-08 - mae: 8.0409e-05 - val_loss: 5.3759e-10 - val_mae: 2.3112e-05\n",
            "Epoch 3/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - loss: 7.6138e-08 - mae: 6.7580e-05 - val_loss: 3.4308e-09 - val_mae: 5.8543e-05\n",
            "Epoch 4/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 8ms/step - loss: 9.3232e-09 - mae: 6.6717e-05 - val_loss: 6.6360e-11 - val_mae: 7.9322e-06\n",
            "Epoch 5/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - loss: 6.0842e-09 - mae: 5.5658e-05 - val_loss: 9.1056e-10 - val_mae: 3.0118e-05\n",
            "Epoch 6/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - loss: 4.6130e-09 - mae: 4.3524e-05 - val_loss: 4.6326e-10 - val_mae: 2.1443e-05\n",
            "Epoch 7/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - loss: 3.8110e-09 - mae: 4.2200e-05 - val_loss: 7.5622e-12 - val_mae: 2.0575e-06\n",
            "Epoch 8/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - loss: 2.0040e-09 - mae: 3.0284e-05 - val_loss: 2.6585e-11 - val_mae: 4.8084e-06\n",
            "Epoch 9/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - loss: 9.8351e-10 - mae: 2.0986e-05 - val_loss: 1.3156e-09 - val_mae: 3.6223e-05\n",
            "Epoch 10/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - loss: 2.5369e-09 - mae: 1.7633e-05 - val_loss: 1.6618e-10 - val_mae: 1.2756e-05\n",
            "Epoch 11/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - loss: 9.4587e-10 - mae: 2.0050e-05 - val_loss: 3.6387e-12 - val_mae: 1.6666e-06\n",
            "Epoch 12/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - loss: 7.3335e-10 - mae: 2.1338e-05 - val_loss: 9.2888e-11 - val_mae: 9.4565e-06\n",
            "Epoch 13/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - loss: 7.4681e-10 - mae: 2.1522e-05 - val_loss: 9.3767e-10 - val_mae: 3.0565e-05\n",
            "Epoch 14/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - loss: 7.1061e-10 - mae: 2.1060e-05 - val_loss: 1.0891e-10 - val_mae: 1.0269e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - loss: 7.2420e-10 - mae: 2.1459e-05 - val_loss: 1.9544e-10 - val_mae: 1.3856e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - loss: 8.9365e-10 - mae: 1.9436e-05 - val_loss: 1.6412e-09 - val_mae: 4.0469e-05\n",
            "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 8.6445e-12 - mae: 1.6838e-06\n",
            "Test MSE: 2.6771709848993908e-11\n",
            "Test MAE: 4.05933178626583e-06\n",
            "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
            "Manual MSE: 2.677170360641461e-11\n",
            "Manual MAE: 4.059332878619283e-06\n"
          ]
        }
      ]
    }
  ]
}