{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SENG 474 Project - Initial Model"
      ],
      "metadata": {
        "id": "C8NfS1aIZR-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Model"
      ],
      "metadata": {
        "id": "7XDvf89guqm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Filepath\n",
        "filepath = \"Kraken_OHLCVT/XBTUSD_15.csv\"\n",
        "\n",
        "# Threshold for determining if a coin went up\n",
        "threshold = 0.004\n",
        "\n",
        "#############################################\n",
        "# 1) Load CSV & Rename columns\n",
        "#############################################\n",
        "df = pd.read_csv(filepath)\n",
        "df.columns = [\"Timestamp\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Trades\"]\n",
        "\n",
        "#############################################\n",
        "# 2) Define feature-engineering functions\n",
        "#############################################\n",
        "def add_datetime_features(df):\n",
        "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"s\")  # Convert to datetime\n",
        "\n",
        "    df[\"Weekday\"] = df[\"Timestamp\"].dt.weekday  # 0=Mon, 6=Sun\n",
        "    df[\"Day\"] = df[\"Timestamp\"].dt.day\n",
        "    df[\"Year\"] = df[\"Timestamp\"].dt.year.astype(float)\n",
        "\n",
        "    # Time of day\n",
        "    df[\"TOD\"] = df[\"Timestamp\"].dt.hour + df[\"Timestamp\"].dt.minute / 60.0\n",
        "\n",
        "    # Cyclical encoding for Month\n",
        "    month = df[\"Timestamp\"].dt.month\n",
        "    df[\"Month_Sin\"] = np.sin(2 * np.pi * month / 12.0)\n",
        "    df[\"Month_Cos\"] = np.cos(2 * np.pi * month / 12.0)\n",
        "\n",
        "    # Cyclical encoding for TOD (24 hours in a day)\n",
        "    df[\"TOD_Sin\"] = np.sin(2 * np.pi * df[\"TOD\"] / 24.0)\n",
        "    df[\"TOD_Cos\"] = np.cos(2 * np.pi * df[\"TOD\"] / 24.0)\n",
        "\n",
        "    # Drop the original columns\n",
        "    df.drop(columns=[\"Timestamp\", \"TOD\"], inplace=True)\n",
        "    return df\n",
        "\n",
        "def compute_rsi(series, period=14):\n",
        "    delta = series.diff().dropna()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / (loss + 1e-10)  # Avoid division by zero\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def add_features(df, threshold):\n",
        "    df[\"SMA_10\"] = df[\"Close\"].rolling(window=10).mean()\n",
        "    df[\"SMA_50\"] = df[\"Close\"].rolling(window=50).mean()\n",
        "    df[\"RSI_14\"] = compute_rsi(df[\"Close\"])\n",
        "    df[\"Return\"] = df[\"Close\"].pct_change()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    df[\"Middle_Band\"] = df[\"Close\"].rolling(window=20).mean()\n",
        "    stddev = df[\"Close\"].rolling(window=20).std()\n",
        "    df[\"Upper_Band\"] = df[\"Middle_Band\"] + (2 * stddev)\n",
        "    df[\"Lower_Band\"] = df[\"Middle_Band\"] - (2 * stddev)\n",
        "\n",
        "    # Return_Signal in {-1, 0, 1}\n",
        "    df[\"Return_Signal\"] = df[\"Return\"].apply(\n",
        "        lambda x: 1 if x > threshold else (0 if x >= 0 else -1)\n",
        "    )\n",
        "\n",
        "    # Add time-based features (which won't leak future data if done carefully)\n",
        "    df = add_datetime_features(df)\n",
        "    return df\n",
        "\n",
        "#############################################\n",
        "# 3) Add features to the full dataframe\n",
        "#############################################\n",
        "df = add_features(df, threshold)\n",
        "\n",
        "#############################################\n",
        "# 4) Split into Train/Val/Test BEFORE fitting scaler\n",
        "#############################################\n",
        "# e.g., 70% train, 15% val, 15% test\n",
        "df = df.reset_index(drop=True)\n",
        "n = len(df)\n",
        "train_end = int(0.7 * n)\n",
        "val_end = int(0.85 * n)\n",
        "\n",
        "train_df = df.iloc[:train_end].copy()\n",
        "val_df = df.iloc[train_end:val_end].copy()\n",
        "test_df = df.iloc[val_end:].copy()\n",
        "\n",
        "#############################################\n",
        "# 5) Fill NaNs using means computed on TRAIN ONLY\n",
        "#############################################\n",
        "train_means = train_df.mean(numeric_only=True)  # means of train set\n",
        "train_df.fillna(train_means, inplace=True)\n",
        "val_df.fillna(train_means, inplace=True)\n",
        "test_df.fillna(train_means, inplace=True)\n",
        "\n",
        "#############################################\n",
        "# 6) Scale using only the TRAIN set\n",
        "#############################################\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Exclude the label \"Return_Signal\" from scaling\n",
        "exclude_cols = [\"Return_Signal\", \"Month_Sin\", \"Month_Cos\", \"TOD_Sin\", \"TOD_Cos\"]\n",
        "features_to_scale = [col for col in train_df.columns if col not in exclude_cols]\n",
        "\n",
        "# Fit scaler on train\n",
        "scaler.fit(train_df[features_to_scale])\n",
        "\n",
        "# Transform train, val, test\n",
        "train_df[features_to_scale] = scaler.transform(train_df[features_to_scale])\n",
        "val_df[features_to_scale]  = scaler.transform(val_df[features_to_scale])\n",
        "test_df[features_to_scale] = scaler.transform(test_df[features_to_scale])\n",
        "\n",
        "# Save scaler\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "#############################################\n",
        "# 7) Reorder columns so that Return_Signal is last\n",
        "#############################################\n",
        "def reorder_columns(df):\n",
        "    cols = list(df.columns)\n",
        "    cols.remove(\"Return_Signal\")\n",
        "    cols.append(\"Return_Signal\")\n",
        "    return df[cols]\n",
        "\n",
        "train_df = reorder_columns(train_df)\n",
        "val_df = reorder_columns(val_df)\n",
        "test_df = reorder_columns(test_df)\n",
        "\n",
        "#############################################\n",
        "# 8) Convert to NumPy & Shift labels {-1,0,1} -> {0,1,2}\n",
        "#############################################\n",
        "def to_numpy_and_shift_labels(df):\n",
        "    data = df.to_numpy()\n",
        "    label_idx = df.columns.get_loc(\"Return_Signal\")\n",
        "    # -1 -> 0\n",
        "    data[:, label_idx] = np.where(data[:, label_idx] == -1, 0, data[:, label_idx])\n",
        "    #  1 -> 2\n",
        "    data[:, label_idx] = np.where(data[:, label_idx] == 1, 2, data[:, label_idx])\n",
        "    return data\n",
        "\n",
        "train_data = to_numpy_and_shift_labels(train_df)\n",
        "val_data = to_numpy_and_shift_labels(val_df)\n",
        "test_data = to_numpy_and_shift_labels(test_df)\n",
        "\n",
        "#############################################\n",
        "# 9) Create sequences (X, y) from each split\n",
        "#############################################\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        # X: the past seq_length rows, all columns except the label\n",
        "        X.append(data[i : i + seq_length, :-1])\n",
        "        # y: the label in the next row\n",
        "        y.append(data[i + seq_length, -1])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 30\n",
        "X_train, y_train = create_sequences(train_data, seq_length)\n",
        "X_val,   y_val   = create_sequences(val_data,   seq_length)\n",
        "X_test,  y_test  = create_sequences(test_data,  seq_length)\n",
        "\n",
        "#############################################\n",
        "# 10) Build LSTM Classification Model\n",
        "#############################################\n",
        "num_features = X_train.shape[2]\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(50, return_sequences=True, input_shape=(seq_length, num_features)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(50, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(25, activation=\"relu\"),\n",
        "    Dense(3, activation=\"softmax\")  # 3 classes: 0=down, 1=neutral, 2=up\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",  # for integer labels\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#############################################\n",
        "# 11) Early Stopping & Training\n",
        "#############################################\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#############################################\n",
        "# 11) Evaluate on Test Set\n",
        "#############################################\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Classification report and confusion matrix\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHmVa4Eruo6c",
        "outputId": "adc75c02-a7a2-475a-eb5a-dfcf20c25474"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 8ms/step - accuracy: 0.8851 - loss: 0.3468 - val_accuracy: 0.9420 - val_loss: 0.2157\n",
            "Epoch 2/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 8ms/step - accuracy: 0.8855 - loss: 0.3236 - val_accuracy: 0.9420 - val_loss: 0.2139\n",
            "Epoch 3/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 8ms/step - accuracy: 0.8860 - loss: 0.3206 - val_accuracy: 0.9420 - val_loss: 0.2090\n",
            "Epoch 4/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 8ms/step - accuracy: 0.8868 - loss: 0.3157 - val_accuracy: 0.9420 - val_loss: 0.2054\n",
            "Epoch 5/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 8ms/step - accuracy: 0.8849 - loss: 0.3172 - val_accuracy: 0.9420 - val_loss: 0.2072\n",
            "Epoch 6/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 8ms/step - accuracy: 0.8854 - loss: 0.3151 - val_accuracy: 0.9420 - val_loss: 0.2128\n",
            "Epoch 7/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 8ms/step - accuracy: 0.8856 - loss: 0.3136 - val_accuracy: 0.9420 - val_loss: 0.2142\n",
            "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9736 - loss: 0.1210\n",
            "Test Loss: 0.1507692188024521\n",
            "Test Accuracy: 0.9622546434402466\n",
            "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      1.00      0.98     45837\n",
            "         2.0       0.00      0.00      0.00      1798\n",
            "\n",
            "    accuracy                           0.96     47635\n",
            "   macro avg       0.48      0.50      0.49     47635\n",
            "weighted avg       0.93      0.96      0.94     47635\n",
            "\n",
            "Confusion Matrix:\n",
            "[[45837     0]\n",
            " [ 1798     0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Report on the Methods and Performance of the LSTM Classification Model**\n",
        "\n",
        "This report outlines the **data preparation**, **feature engineering**, and **model training** steps used to predict whether the price of a cryptocurrency (or similar asset) will go **down (0)**, remain **neutral (1)**, or go **up (2)**. The code leverages a **Long Short-Term Memory (LSTM)** architecture, a type of recurrent neural network well-suited for sequential data.\n",
        "\n",
        "Following training, we evaluate the model’s performance and discuss key observations of the results.\n",
        "\n",
        "\n",
        "## 2. Methodology\n",
        "\n",
        "### 2.1 Data Ingestion and Setup\n",
        "1. **File Loading:**  \n",
        "   - The CSV file (`\"Kraken_OHLCVT/XBTUSD_15.csv\"`) is read into a Pandas DataFrame `df`.\n",
        "   - Columns are renamed to `[\"Timestamp\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Trades\"]`.\n",
        "\n",
        "2. **Threshold Definition:**  \n",
        "   - A `threshold` of `0.004` is chosen for determining whether the next price movement is considered an **up** movement (`return > 0.004`).\n",
        "\n",
        "\n",
        "### 2.2 Feature Engineering\n",
        "1. **Technical Indicators:**\n",
        "   - **Simple Moving Averages (SMA_10, SMA_50):** 10- and 50-period means of `Close`.\n",
        "   - **RSI_14:** 14-period Relative Strength Index, measuring momentum.\n",
        "   - **Bollinger Bands (Upper, Middle, Lower):** 20-period average ± 2 standard deviations.\n",
        "   - **Return:** Percentage change of `Close` from one time step to the next.\n",
        "\n",
        "2. **Date-Time Features (`add_datetime_features`):**  \n",
        "   - Convert `Timestamp` to a Python `datetime`.  \n",
        "   - Extract **Weekday** (0=Monday to 6=Sunday), **Day**, **Year**, and **Time-of-Day (TOD)**.  \n",
        "   - Apply **cyclical encodings** for `Month` (`Month_Sin`, `Month_Cos`) and `TOD` (`TOD_Sin`, `TOD_Cos`) so that cyclical patterns are preserved (e.g., 23:59 is close to 00:00).\n",
        "\n",
        "3. **Return_Signal:**  \n",
        "   - Categorize each time step as **-1** (down) if `return < 0`, **0** (neutral) if `0 <= return <= threshold`, or **1** (up) if `return > threshold`.  \n",
        "   - Later, these values are remapped from \\{-1, 0, 1\\} → \\{0, 1, 2\\} for compatibility with Keras classification layers.\n",
        "\n",
        "\n",
        "### 2.3 Splitting the Dataset\n",
        "1. **Chronological Splits:**  \n",
        "   - **Train set (70%)**: The earliest portion of the time series.  \n",
        "   - **Validation set (15%)**: The next portion. Used for tuning hyperparameters (via early stopping) and checking generalization during training.  \n",
        "   - **Test set (15%)**: The final portion. Used **after** training to report final performance.\n",
        "\n",
        "2. **Avoiding Data Leakage:**  \n",
        "   - We reset indexes and fill missing values with the **train set means** only.  \n",
        "   - We also **fit the scaler** (MinMax) on the train set and **apply** it to the validation and test sets.\n",
        "\n",
        "\n",
        "### 2.4 Preprocessing and Scaling\n",
        "1. **Handling Missing Data:**  \n",
        "   - We compute `train_means` from numerical columns in the **train set** and use these means to fill any missing values in train, val, and test sets.\n",
        "2. **MinMaxScaler:**  \n",
        "   - Fits to the **train set** only, then applies transforms to val/test.  \n",
        "   - Columns like `Return_Signal`, `Month_Sin`, `Month_Cos`, `TOD_Sin`, `TOD_Cos` are **excluded** from scaling since they are either labels or already in the range \\([-1,1]\\).\n",
        "\n",
        "3. **Reordering Columns:**  \n",
        "   - Ensures `Return_Signal` is the last column for convenient indexing, letting everything else act as features.\n",
        "\n",
        "4. **Label Shifting:**  \n",
        "   - \\{-1 → 0, +1 → 2\\} leaves us with final integer labels \\{0,1,2\\}, suitable for **`sparse_categorical_crossentropy`**.\n",
        "\n",
        "\n",
        "### 2.5 Sequence Creation\n",
        "1. **Sliding Window (`create_sequences`)**:  \n",
        "   - We use a **sequence length** of `seq_length=30`.  \n",
        "   - For each index \\(i\\), \\(X_i\\) is the **past 30 rows** of features, and \\(y_i\\) is the **`Return_Signal`** (remapped) on the **31st row**.  \n",
        "   - This forms a dataset of shape \\((\\text{samples}, 30, \\text{num_features})\\).\n",
        "\n",
        "\n",
        "### 2.6 Model Definition and Training\n",
        "1. **LSTM Architecture**  \n",
        "   - A two-layer LSTM:\n",
        "     1. **LSTM(50, return_sequences=True)** + **Dropout(0.2)**\n",
        "     2. **LSTM(50, return_sequences=False)** + **Dropout(0.2)**\n",
        "   - A **Dense(25, \"relu\")** hidden layer.\n",
        "   - A final **Dense(3, \"softmax\")** for 3-class classification: \\{down=0, neutral=1, up=2\\}.\n",
        "\n",
        "2. **Compilation**  \n",
        "   - **Optimizer:** `\"adam\"`\n",
        "   - **Loss:** `\"sparse_categorical_crossentropy\"` (for integer labels)\n",
        "   - **Metrics:** `[\"accuracy\"]` for classification accuracy.\n",
        "\n",
        "3. **Early Stopping**  \n",
        "   - **patience=3**, monitoring `val_loss`.\n",
        "   - Automatically **restores best weights** to avoid overfitting if the validation loss stops improving.\n",
        "\n",
        "4. **Fitting**  \n",
        "   - `epochs=50, batch_size=32`\n",
        "   - Validation data: `(X_val, y_val)`\n",
        "\n",
        "\n",
        "## 3. Model Evaluation and Observations\n",
        "\n",
        "### 3.1 Final Accuracy and Loss on Test Set\n",
        "- After training, the model reports:  \n",
        "  - **Test Loss** around **0.15**  \n",
        "  - **Test Accuracy** around **0.96** (96%)\n",
        "\n",
        "On the surface, **96% accuracy** seems impressive. However, further investigation via the **classification report** and **confusion matrix** reveals:\n",
        "\n",
        "```\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         0.0       0.96      1.00      0.98     45837\n",
        "         2.0       0.00      0.00      0.00      1798\n",
        "\n",
        "    accuracy                           0.96     47635\n",
        "   macro avg       0.48      0.50      0.49     47635\n",
        "weighted avg       0.93      0.96      0.94     47635\n",
        "\n",
        "Confusion Matrix:\n",
        "[[45837     0]\n",
        " [ 1798     0]]\n",
        "```\n",
        "\n",
        "- **Class 0** (down) is predicted **45837 times** correctly and **0** times incorrectly.  \n",
        "- **Class 2** (up) was **never predicted** (0 times), even though 1798 samples are truly class 2.  \n",
        "- **Class 1** (neutral) doesn’t appear in the table, which suggests none of the final test rows were labeled 1 in this particular sample.  \n",
        "\n",
        "Thus, the model **mostly predicts one class** (class 0) in the test set, ignoring the minority class(es). This can happen when:\n",
        "\n",
        "1. The data is **highly imbalanced** (far more “down” than “up” signals).  \n",
        "2. The threshold of `0.004` might be too large or too small, leading to few “up” examples.  \n",
        "3. The model’s loss function with a large class imbalance may lead it to favor the majority class.\n",
        "\n",
        "\n",
        "### 3.2 Conclusions and Future Work\n",
        "- **High Accuracy but Class Imbalance:** A ~96% accuracy is misleading because the model rarely predicts the “up” class. It appears it learned to always predict “down” for minimal penalty.\n",
        "- **Potential Remedies:**\n",
        "  1. **Class Weights:** Give more weight to minority classes during training to encourage the model to learn them.  \n",
        "  2. **Data Balancing:** Either oversample “up” or undersample “down,” or use SMOTE-like techniques if the dataset is highly imbalanced.  \n",
        "  3. **Tune Threshold:** Adjust the threshold (0.004) if it’s causing too few “up” labels.  \n",
        "  4. **More Features:** Possibly incorporate additional signals that help differentiate “down” vs. “up.”\n",
        "\n",
        "Overall, the pipeline for data engineering and LSTM modeling is sound, but the **results highlight class imbalance**. Improving class balance or weighting could lead to more meaningful predictions across all three categories.\n",
        "\n",
        "\n",
        "## 4. Summary\n",
        "1. **Pipeline**: Data is **chronologically split** into train/validation/test sets, we carefully engineer features (SMA, RSI, Bollinger, cyclical time features, etc.), then form a **sequence** dataset for the LSTM.  \n",
        "2. **Model**: A **two-layer LSTM** with dropout and a dense output layer for **3-class classification**.  \n",
        "3. **Observations**: Although the model achieves **high accuracy**, it **ignores the minority class** in practice, revealing **class imbalance**.  \n",
        "4. **Next Steps**: Adjust thresholds, apply class weighting, or resample data to ensure all labels are properly learned.\n",
        "\n",
        "This approach demonstrates how an LSTM can handle **sequence classification** for financial time series, but also shows the critical importance of evaluating class distributions and exploring deeper metrics (confusion matrix, F1-scores) to ensure fair performance across all classes."
      ],
      "metadata": {
        "id": "75-9tWbK1D5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Yf-5UEdb1BZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model learned that only predicting \"down\" is correct. We verify that this could be an issue (and the issue is not with preprocessing):"
      ],
      "metadata": {
        "id": "Bchy9bL_zlNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Provide your file path\n",
        "filepath = \"Kraken_OHLCVT/XBTUSD_15.csv\"\n",
        "\n",
        "# Threshold\n",
        "threshold = 0.004\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(filepath, names=[\"Timestamp\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Trades\"], header=None)\n",
        "\n",
        "# Add the 'Return' column (pct change of Close)\n",
        "df[\"Return\"] = df[\"Close\"].pct_change()\n",
        "\n",
        "# Derive the Return_Signal in {-1,0,1} using the same threshold logic\n",
        "def classify_return(x, threshold=0.004):\n",
        "    if pd.isna(x):\n",
        "        return None  # or 0, if you want to fill NaNs differently\n",
        "    elif x > threshold:\n",
        "        return 1\n",
        "    elif x >= 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "df[\"Return_Signal\"] = df[\"Return\"].apply(lambda x: classify_return(x, threshold))\n",
        "\n",
        "# Drop the first row if 'Return' was NaN after pct_change\n",
        "df.dropna(subset=[\"Return_Signal\"], inplace=True)\n",
        "\n",
        "# Count the distribution\n",
        "distribution = df[\"Return_Signal\"].value_counts()\n",
        "print(\"Distribution of Return_Signal:\\n\", distribution)\n",
        "\n",
        "# Percentage breakdown\n",
        "distribution_pct = df[\"Return_Signal\"].value_counts(normalize=True) * 100\n",
        "print(\"\\nPercentage Distribution:\\n\", distribution_pct)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDXnCu5FzucG",
        "outputId": "21464898-ed31-4986-9249-759f00f328cd"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution of Return_Signal:\n",
            " Return_Signal\n",
            "-1.0    153036\n",
            " 0.0    134806\n",
            " 1.0     29923\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentage Distribution:\n",
            " Return_Signal\n",
            "-1.0    48.160118\n",
            " 0.0    42.423174\n",
            " 1.0     9.416707\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We retry with class weights:"
      ],
      "metadata": {
        "id": "aq6MndqI0hAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Suppose y_train contains labels in {0,1,2} with class imbalance\n",
        "# e.g., y_train = np.array([0,0,2,0,1,2,0,0,0, ... ])\n",
        "\n",
        "# 1) Count how many samples per class in the training set\n",
        "counter = Counter(y_train)  # e.g., Counter({0: 153036, 1: 134806, 2: 29923})\n",
        "print(\"Training Distribution:\", counter)\n",
        "\n",
        "# 2) Compute class weights (Inverse Frequency Example)\n",
        "# total_samples = sum of all classes\n",
        "total_samples = len(y_train)\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "class_weight = {}\n",
        "for label, count in counter.items():\n",
        "    # weight_i = total_samples / (num_classes * class_count_i)\n",
        "    class_weight[label] = total_samples / (num_classes * count)\n",
        "\n",
        "print(\"Class Weight Dictionary:\", class_weight)\n",
        "# Example output: {0: 1.39, 1: 1.58, 2: 7.12}  (numbers are hypothetical)\n",
        "\n",
        "# 3) Build a model (example: LSTM) for classification\n",
        "model = Sequential([\n",
        "    LSTM(50, return_sequences=True, input_shape=(seq_length, num_features)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(50, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(25, activation=\"relu\"),\n",
        "    Dense(3, activation=\"softmax\")  # 3 classes: 0,1,2\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",  # for integer labels\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# 4) Apply Early Stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# 5) Train with class_weight\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1,\n",
        "    class_weight=class_weight  # <--- here we pass the weights\n",
        ")\n",
        "\n",
        "# 6) Evaluate on test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXa8X_zd0nmn",
        "outputId": "8fc14efc-b1ab-4215-ebef-51811078fd18"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Distribution: Counter({0.0: 197063, 2.0: 25342})\n",
            "Class Weight Dictionary: {0.0: 0.5642992342550351, 2.0: 4.388071186173152}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 8ms/step - accuracy: 0.5960 - loss: 0.6668 - val_accuracy: 0.7423 - val_loss: 0.5355\n",
            "Epoch 2/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - accuracy: 0.6521 - loss: 0.6113 - val_accuracy: 0.8174 - val_loss: 0.4673\n",
            "Epoch 3/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 8ms/step - accuracy: 0.6525 - loss: 0.6028 - val_accuracy: 0.8112 - val_loss: 0.4726\n",
            "Epoch 4/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 8ms/step - accuracy: 0.6546 - loss: 0.6002 - val_accuracy: 0.7314 - val_loss: 0.5187\n",
            "Epoch 5/50\n",
            "\u001b[1m6951/6951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 8ms/step - accuracy: 0.6588 - loss: 0.5954 - val_accuracy: 0.8103 - val_loss: 0.4868\n",
            "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9200 - loss: 0.4038\n",
            "Test Loss: 0.4121701419353485\n",
            "Test Accuracy: 0.9291487336158752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjCgmD2i12zl",
        "outputId": "2ab413c2-ca82-4a5d-cae7-dfa7d0bde4f8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      1.00      0.98     45837\n",
            "         2.0       0.00      0.00      0.00      1798\n",
            "\n",
            "    accuracy                           0.96     47635\n",
            "   macro avg       0.48      0.50      0.49     47635\n",
            "weighted avg       0.93      0.96      0.94     47635\n",
            "\n",
            "Confusion Matrix:\n",
            "[[45837     0]\n",
            " [ 1798     0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Report on Using Class Weights for an Imbalanced LSTM Classification Task**\n",
        "\n",
        "\n",
        "This report describes how **class weighting** was introduced into an LSTM classification pipeline to address **imbalanced training data**, where one class (“0.0”) heavily outnumbered another class (“2.0”). The goal is to enable the model to learn minority classes more effectively.\n",
        "\n",
        "Despite using class weights, the final confusion matrix and classification report show the model **still** overwhelmingly predicts the majority class. Below, we discuss the approach and evaluate the resulting performance.\n",
        "\n",
        "\n",
        "## **2. Methodology**\n",
        "\n",
        "1. **Data**:  \n",
        "   - The training labels \\(\\{0, 1, 2\\}\\) exhibit significant imbalance.  \n",
        "   - Example distribution: \\(\\text{Counter}(\\{0.0: 197063, 2.0: 25342\\})\\).  \n",
        "   - We focus here on classes **0.0** and **2.0** (no class 1.0 in the distribution snippet).\n",
        "\n",
        "2. **Computing Class Weights**:  \n",
        "   - **Inverse Frequency Formula**:\n",
        "     \\[\n",
        "       \\text{weight}_i = \\frac{\\text{total\\_samples}}{\\text{num\\_classes} \\times \\text{class\\_count}_i}\n",
        "     \\]\n",
        "   - This results in a higher weight for minority classes, penalizing errors for smaller classes more heavily.\n",
        "\n",
        "3. **Model Architecture**:  \n",
        "   - **Two-layer LSTM** network with 50 units each, plus dropout for regularization.  \n",
        "   - Output layer: `Dense(3, activation=\"softmax\")` for a 3-class classification.\n",
        "\n",
        "4. **Training with `class_weight`**:  \n",
        "   - Passed a `class_weight` dictionary into `model.fit(...)`.  \n",
        "   - Also employed **early stopping** (`patience=3`), monitoring validation loss.\n",
        "\n",
        "\n",
        "## **3. Training Results**\n",
        "\n",
        "### **3.1 Training Distribution and Weights**\n",
        "\n",
        "```\n",
        "Training Distribution: Counter({0.0: 197063, 2.0: 25342})\n",
        "Class Weight Dictionary: {0.0: 0.5643, 2.0: 4.3881}\n",
        "```\n",
        "\n",
        "- Class **0.0** (majority) got a weight of ~0.56.  \n",
        "- Class **2.0** (minority) got a weight of ~4.39.  \n",
        "\n",
        "This indicates the minority class’ errors should be penalized more.\n",
        "\n",
        "### **3.2 Epoch Logs**\n",
        "\n",
        "```\n",
        "Epoch 1/50\n",
        "...\n",
        "accuracy: 0.5960 - loss: 0.6668\n",
        "val_accuracy: 0.7423 - val_loss: 0.5355\n",
        "\n",
        "Epoch 2/50\n",
        "...\n",
        "accuracy: 0.6521 - loss: 0.6113\n",
        "val_accuracy: 0.8174 - val_loss: 0.4673\n",
        "\n",
        "...\n",
        "Epoch 5/50\n",
        "...\n",
        "accuracy: 0.6588 - loss: 0.5954\n",
        "val_accuracy: 0.8103 - val_loss: 0.4868\n",
        "```\n",
        "\n",
        "- The model steadily **increased** accuracy on the training set (from ~59.6% to ~65.9%).  \n",
        "- Validation accuracy rose to above 80% by epoch 2, then fluctuated slightly.\n",
        "\n",
        "### **3.3 Final Test Performance**\n",
        "\n",
        "```\n",
        "Test Loss: 0.4122\n",
        "Test Accuracy: 0.9291\n",
        "```\n",
        "\n",
        "- **Test accuracy** is around **92.9%**.  \n",
        "- On the surface, this is high. However, a closer look at the confusion matrix reveals the issue with the minority class.\n",
        "\n",
        "\n",
        "## **4. Model Evaluation**\n",
        "\n",
        "### **4.1 Confusion Matrix and Classification Report**\n",
        "\n",
        "```\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         0.0       0.96      1.00      0.98     45837\n",
        "         2.0       0.00      0.00      0.00      1798\n",
        "\n",
        "Confusion Matrix:\n",
        "[[45837     0]\n",
        " [ 1798     0]]\n",
        "```\n",
        "\n",
        "- The model **never** predicted “2.0,” resulting in:\n",
        "  - 100% correct predictions for class “0.0” (all 45,837).\n",
        "  - **0** correct predictions for class “2.0.”  \n",
        "- The **model simply learned to predict the majority class** every time.\n",
        "\n",
        "### **4.2 Interpretation**\n",
        "\n",
        "Even though we used **class weights**, the model still defaulted to predicting the majority class (“0.0”) exclusively. Potential reasons include:\n",
        "\n",
        "1. **Imbalance is Still High**: A ratio of ~197k to ~25k is quite large, and while class weighting helps, it may not be enough.  \n",
        "2. **Loss Minimization**: The model can still minimize overall loss by predicting the majority class, especially if the minority class signals are weak or not well-differentiated.  \n",
        "3. **Insufficient Minority Examples**: If the minority class is only ~11% of data, the model might need additional strategies to see enough examples.\n",
        "\n",
        "\n",
        "## **5. Conclusion and Recommendations**\n",
        "\n",
        "1. **High Accuracy ≠ Good Minority Detection**  \n",
        "   - A 92.9% test accuracy can be misleading when the minority class is completely unrecognized.\n",
        "\n",
        "2. **Improving Minority Class Recognition**  \n",
        "   - **Increase Class Weights Further**: The current inverse frequency approach gave a weight of ~4.39 to class “2.0.” Possibly even higher weights or manual tuning might help.  \n",
        "   - **Resampling / SMOTE**: Oversample minority examples or undersample the majority to achieve a more balanced dataset.  \n",
        "   - **Additional Features / Different Threshold**: If “2.0” is defined by a threshold, reconsider the threshold or provide stronger signals that differentiate “up” from “down.”  \n",
        "   - **Use Other Metrics**: Track **F1** for each class, especially the minority. Then tune until you see improvement for class “2.0.”  \n",
        "\n",
        "3. **Next Steps**  \n",
        "   - **Try combining** class weights with oversampling (e.g., SMOTE).  \n",
        "   - **Examine** whether features truly capture the upward price signals.  \n",
        "   - **Assess** real-world cost: If missing an “up” signal is costly, weighting the minority class more aggressively can be justified.\n",
        "\n",
        "Despite best efforts with class weighting, the model remains dominated by the majority class. By integrating more advanced imbalance-handling techniques, you can encourage predictions for minority classes and achieve a more balanced performance overall."
      ],
      "metadata": {
        "id": "3FF5OzuA2i_R"
      }
    }
  ]
}